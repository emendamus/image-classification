{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Klassifikation von Bildern\n",
    "\n",
    "## Daten laden\n",
    "Daten werden von folgender URL geladen(https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Konfiguration\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereiten der Daten\n",
    "\n",
    "### Normalisieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x/np.max(x, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    arr = np.array(x)\n",
    "    zeroarray = np.zeros((len(x), 10))\n",
    "\n",
    "    for i,label in enumerate(arr):   \n",
    "       zeroarray[i,label] = 1 \n",
    "    return zeroarray\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereiten und Speichern der Daten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    return tf.placeholder(tf.float32, shape=(None,image_shape[0],image_shape[1],image_shape[2]),name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    return tf.placeholder(tf.float32, shape=(None,n_classes),name=\"y\" )\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution und Max Pooling Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides,layer_count):\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], int(x_tensor.get_shape()[3]), conv_num_outputs],stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    max_pool_ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    max_pool_strides = [1, pool_strides[0], pool_strides[1], 1]\n",
    "    maxpool = tf.nn.max_pool(conv_layer, max_pool_ksize, max_pool_strides, padding='SAME')\n",
    "    \n",
    "    #tf.summary.histogram(layer_count+'_maxpool_w', weight)\n",
    "    #tf.summary.histogram(layer_count+'_maxpool_b', bias)\n",
    "    \n",
    "    return  maxpool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    return tf.contrib.layers.flatten(x_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor,num_outputs=num_outputs,activation_fn=tf.nn.relu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    return tf.contrib.layers.fully_connected(inputs=x_tensor,num_outputs=num_outputs,activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob,cnn_layer=1):\n",
    "    maxpool_layer =  conv2d_maxpool(x, 32, (2,2), (2,2), (2,2), (2,2),\"1\")\n",
    "    \n",
    "    if(cnn_layer == 2 or cnn_layer == 3):\n",
    "        maxpool_layer =  conv2d_maxpool(maxpool_layer, 64, (2,2), (2,2), (2,2), (2,2),\"2\")\n",
    "    \n",
    "    if(cnn_layer == 3):\n",
    "        maxpool_layer =  conv2d_maxpool(maxpool_layer, 128, (2,2), (2,2), (2,2), (2,2),\"3\")\n",
    "    \n",
    "    dropout_layer = tf.nn.dropout(x=maxpool_layer,keep_prob=keep_prob)\n",
    "\n",
    "    flatten_layer = flatten(dropout_layer)\n",
    "\n",
    "    fully_connected_layer = fully_conn(flatten_layer,128)\n",
    "    \n",
    "    fully_connected_layer = fully_conn(fully_connected_layer,64)\n",
    "    \n",
    "    output_layer = output(fully_connected_layer,10)\n",
    "   \n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "\n",
    "def build_cnn(cnn_layer):\n",
    "    # Remove previous weights, bias, inputs, etc..\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Inputs\n",
    "    with tf.name_scope('inputs'):\n",
    "        x = neural_net_image_input((32, 32, 3))\n",
    "        y = neural_net_label_input(10)\n",
    "        keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "    # Model\n",
    "    with tf.name_scope('model'):\n",
    "        logits = conv_net(x, keep_prob,cnn_layer)\n",
    "        logits = tf.identity(logits, name='logits')\n",
    "    \n",
    "    # Loss and Optimizer\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "\n",
    "    # Accuracy\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "        tf.summary.scalar('accuracy', accuracy)    \n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    model = {}\n",
    "    model[\"optimizer\"] = optimizer\n",
    "    model[\"cost\"] = cost\n",
    "    model[\"accuracy\"] = accuracy\n",
    "    model[\"merged\"] = merged\n",
    "    model[\"x\"] = x\n",
    "    model[\"y\"] = y\n",
    "    model[\"keep_prob\"] = keep_prob\n",
    "\n",
    "    return model\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Neural Network\n",
    "\n",
    "### Single Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, keep_probability, feature_batch, label_batch,iteration,batchsize,model,cnn_layer,epoch):\n",
    "    log_string = './output/batchsize={},epochs={},cnn_layer={}'.format(batchsize, epoch,cnn_layer)\n",
    "    file_writer = tf.summary.FileWriter(log_string, sess.graph)\n",
    "    train_feed_dict = {\n",
    "                model[\"x\"]: feature_batch,\n",
    "                model[\"y\"]: label_batch,\n",
    "                model[\"keep_prob\"]: keep_probability}\n",
    "    summary , _= session.run([model[\"merged\"],model[\"optimizer\"]], feed_dict=train_feed_dict)\n",
    "       \n",
    "    file_writer.add_summary(summary, iteration)\n",
    "    file_writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistik\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, model):\n",
    "    loss = session.run(model[\"cost\"], feed_dict={model[\"x\"]: feature_batch, model[\"y\"]: label_batch, model[\"keep_prob\"]: 1})\n",
    "    valid = sess.run(model[\"accuracy\"], feed_dict={model[\"x\"]: valid_features, model[\"y\"]: valid_labels, model[\"keep_prob\"]: 1})\n",
    "    print('Loss: {:10.4f} Validation Accuracy: {:.6f}'.format(loss, valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 128\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "Mit allen Batches trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "for batch_size in [128,256]:\n",
    "    for epochs in [2,10]:\n",
    "        for cnn_layer in [1,3]:\n",
    "            model = build_cnn(cnn_layer)\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                \n",
    "                for epoch in range(epochs):\n",
    "                    # Loop over all batches\n",
    "                    n_batches = 5\n",
    "                    for batch_i in range(1, n_batches + 1):\n",
    "                        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                            train_neural_network(sess, keep_probability, batch_features, batch_labels,epoch*batch_i+1,batch_size,model,cnn_layer,epochs)\n",
    "                        print('Epoch {:>2}, CIFAR-10 Batch {}:  ,Batch-Size {},Epochs {},CNN-Layer {} '.format(epoch + 1, batch_i, batch_size,epochs,cnn_layer), end='')\n",
    "                        print_stats(sess, batch_features, batch_labels, model)\n",
    "            \n",
    "                # Save Model\n",
    "                saver = tf.train.Saver()\n",
    "                save_path = saver.save(sess, save_model_path+str(batch_size)+str(epochs)+str(cnn_layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "Das Model wird persistiert.\n",
    "## Test Model\n",
    "Das Model wird mit den Testdaten getestet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "               \n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('inputs/x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('inputs/y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('inputs/keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('model/logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy/accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
